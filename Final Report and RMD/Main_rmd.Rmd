---
title: "678 Midterm Project Main"
author: "Qianhui Rong"
date: "11/12/2018"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,warning=FALSE}
#Library
library(MASS)
library(dplyr)
library(magrittr)
library(ggplot2)
library(cluster)
library(gridExtra)
library(lme4)
library(party)
library(arm)
library(MLmetrics)
library(logistf)
library(brglm)
library(merTools)
library(corrplot)
library(GGally)
```

```{r}
#Load data 
data_whole <- read.table(file = 'content_train.tsv', sep = '\t', header = TRUE)
#Add two columns: total_spend and total_no
data_whole%<>% mutate(total_spend=express.total.spend+metro.total.spend+
                      superstore.total.spend+extra.total.spend+fandf.total.spend+
                       petrol.total.spend+direct.total.spend)
data_whole%<>% mutate(total_no=express.no.transactions+metro.no.transactions+
                    superstore.no.transactions+extra.no.transactions+
                      fandf.no.transactions+
                      petrol.no.transactions+direct.no.transactions)
#Seperate the data into train set and test set (80:20 split)
#Random generate 100000*0.2 test set
set.seed(2018)
test_index <-  sort(sample(nrow(data_whole), nrow(data_whole)*.2))
data_test <- data_whole[test_index,]
data_train <- data_whole[-test_index,]
```


#A. Abstract

This analysis project is conducted based on the Tesco purchase history data. The goal is to build an appropriate model to predict the advertisements' click-through-rate for each of the customers in Tesco's database. This has been done with multinomial model, logistic model and multilevel mixed effect model. A problem named "rare event" problem or "rare event problem" has arose during modeling process. Some methods have been used to tackle this problem, but their performance is not up to the expectation, which leaves a wide exploring space for future analysis on this data. 


#B. Introduction

Click-through_rate(CTR) is the percentage rate at which people click on a particular ad when online. A critical step to improve CTR is to target the right group of customers for each kind of advertisement. Therefore, exploring the relation between features of customers (for example, demographic characteristics and purchase preferences) and whether they have clicked through this advertisement is important.
The most common and efficient method is logistic regression, predicting the probability of a certain group of people clicking a link. Also, there are some machine learning methods popular for this problem: Gradient Boosting Decision Tree, Factorization Mechines, etc.


#C. Method

##1. Data source

The data is obtained from https://www.kaggle.com/linkonabe/tesco-marketing-content. It's shared by Tesco company, which is a supermarket market leader in the UK. 
The company releases some marketing content cards each year, and in this dataset, we have 9 of them marked as content_1 to content_9. For each content card, 1 means the customer clicked on the card, 0 means the customer viewed the card but didn’t click, NA means the user was never shown the card.
The dataset also shares its customers' purchase history in different kinds of shop. There are seven kinds of store: 
-Tesco Express: neighbourhood convenience shops
-Tesco Metro: located in city centres beside railway stations
-Tesco Superstore: standard large supermarkets
-Tesco Extra: larger, mainly out-of-town hypermarkets
-Tesco F&F: online store selling Tesco's own clothes brand
-Tesco Direct: online store selling groceries, homewares, electronics, etc.
-Tesco Petrol: grocery store in petrol stations
Some customers' demographic features are also provided in the data: gender and county. Affluency is a more informative variable than county. It's a broad categorisation of how affluent the customer is based on their postcode.

##2. Model used

Models selected to explain the content_1 variable are:

-Multinomial Model: using content_1 without conversion or pre-processing, and keep its three levels(0,1,NA);

For the following two models, preprocessing is necessary. I've first tried out converting NA to 0 and leave 1 as 1, so content_1 becomes "absence of click" vs. "presence of click", but this makes the "rare event" problem more severe(in appendix). Then I switched to eliminating the NA directly in both training set and testing set, in order to make "ones" in content_1 to have a higher proportion, compared to "zeros" in content_1(in main result section).
-Logistic Model
-Multilevel Mixed Effect Model


#D. Results

##Part I: EDA
###EDA on Affluency 

About affluency, I'm expecting a higher amount of transactions and a higher number of transactions in regions of higher degree of affluency. 

###1. Total amount of transactions vs. Affluency
```{r}
data_train %>% group_by(affluency) %>% 
  ggplot(aes(x=as.factor(affluency),y=total_spend))+ geom_boxplot()+
  scale_x_discrete(limits=c("Very Low","Low","Mid","High","Very High"))+
  xlab("Affluency") +
  ylab("Total Spend") +
  coord_flip()+
  labs(title="Total Spend Amount vs. Affluency",subtitle="No apparent difference in total spend range for each affulency class")

#There are some points located in higher spend amount for Mid class affluency, but these points are trivial to the whole data size.

#Double check if those five bar charts are that close
#c(236496799/19772,242749658/20133,601268557/50105,60063877/5009,59859247/4981)
```

###2. Total Number of transactions vs. Affluency
```{r}
data_train %>% group_by(affluency) %>% 
  ggplot(aes(x=as.factor(affluency),y=total_no))+ geom_boxplot()+
  scale_x_discrete(limits=c("Very Low","Low","Mid","High","Very High"))+
  xlab("Affluency") +
  ylab("Total Number of Transactions") +
  coord_flip()+
  labs(title="Total Number of Transactions vs. Affluency",subtitle="No apparent difference in number of transactions range for each affulency class")
```

###3. Clicking in Different Content (1~9) vs. Affluency
```{r}
#Group data
plot_mat <- matrix(0,nrow =45,ncol=3)
colnames(plot_mat) <- c("Content","Affluency","Count")
plot_mat[,1] <- c(rep("content_1",5),rep("content_2",5),rep("content_3",5),rep("content_4",5),
                  rep("content_5",5),rep("content_6",5),rep("content_7",5),rep("content_8",5),
                  rep("content_9",5))
plot_mat[,2] <- rep(c("High","Low","Mid","Very High","Very Low"),9)

mat<- matrix(0,nrow = 9,ncol=5)
for (i in 1:9){
  vh <- 0
  h <- 0
  m <- 0
  l <- 0
  vl <- 0
  vh <- length(which(data_train[which(data_train[,1+i]==1),26]=="Very High")==TRUE)
  h <- length(which(data_train[which(data_train[,1+i]==1),26]=="High")==TRUE)
  m <- length(which(data_train[which(data_train[,1+i]==1),26]=="Mid")==TRUE)
  l <- length(which(data_train[which(data_train[,1+i]==1),26]=="Low")==TRUE)
  vl <- length(which(data_train[which(data_train[,1+i]==1),26]=="Very Low")==TRUE)
  mat[i,] <- c(h,l,m,vh,vl)
}
mat <- t(mat)
mat <- as.data.frame(mat)
rownames(mat) <- c("High","Low","Mid","Very High","Very Low")
colnames(mat) <- c("1","2","3","4","5","6","7","8","9")
plot_mat[,3] <- c(mat$`1`,mat$`2`,mat$`3`,mat$`4`,mat$`5`,mat$`6`,mat$`7`,mat$`8`,mat$`9`)
plot_mat <- as.data.frame(plot_mat)

#Plot
plot_mat$Count <- as.numeric(plot_mat$Count)
ggplot(data = plot_mat)+
  aes(x=as.factor(Content),y=Count,fill=as.factor(Affluency))+
  geom_col(position = 'stack')+
  xlab("Content 1 to 9")+
  scale_fill_discrete(name="Affluency")+
  labs(title="Count number of clicking each content vs. Affluency")

```


##EDA on Gender

###1. Total amount of transactions vs. Gender
```{r}
data_train %>% group_by(gender) %>% 
  ggplot(aes(x=as.factor(gender),y=total_spend))+ geom_boxplot()+
  xlab("Gender") +
  ylab("Total Spend") +
  coord_flip()+
  labs(title="Total Spend Amount vs. Gender",subtitle="No apparent difference in total spend range for each gender")
```

###2. Total Number of transactions vs. Gender
```{r}
data_train %>% group_by(gender) %>% 
  ggplot(aes(x=as.factor(gender),y=total_no))+ geom_boxplot()+
  xlab("Gender") +
  ylab("Total Number of Transactions") +
  coord_flip()+
  labs(title="Total Number of Transactions vs. Gender",subtitle="No apparent difference in number of transactions range for each gender")
```

###3. Clicking in Different Content (1~9) vs. Gender
```{r}
#Group data
plot_mat <- matrix(0,nrow =18,ncol=3)
colnames(plot_mat) <- c("Content","Gender","Count")
plot_mat[,1] <- c(rep("content_1",2),rep("content_2",2),rep("content_3",2),rep("content_4",2),
                  rep("content_5",2),rep("content_6",2),rep("content_7",2),rep("content_8",2),
                  rep("content_9",2))
plot_mat[,2] <- rep(c("Male","Female"),9)

mat<- matrix(0,nrow = 9,ncol=2)
for (i in 1:9){
  f <- 0
  m <- 0
  m <- length(which(data_train[which(data_train[,1+i]==1),25]=="Male")==TRUE)
  f <- length(which(data_train[which(data_train[,1+i]==1),25]=="Female")==TRUE)
  mat[i,] <- c(m,f)
}
mat <- t(mat)
mat <- as.data.frame(mat)
rownames(mat) <- c("Male","Female")
colnames(mat) <- c("1","2","3","4","5","6","7","8","9")
plot_mat[,3] <- c(mat$`1`,mat$`2`,mat$`3`,mat$`4`,mat$`5`,mat$`6`,mat$`7`,mat$`8`,mat$`9`)
plot_mat <- as.data.frame(plot_mat)

#Plot
plot_mat$Count <- as.numeric(plot_mat$Count)
ggplot(data = plot_mat)+
  aes(x=as.factor(Content),y=Count,fill=as.factor(Gender))+
  geom_col(position = 'stack')+
  xlab("Content 1 to 9")+
  scale_fill_discrete(name="Gender")+
  labs(title="Count number of clicking each content vs. Gender")
```


##EDA on Contents(Clicked/Ignored/Not Informed)
```{r}
#Calculate the number of click for each customer
for (i in 1:80000){data_train[i,30]<- sum(as.numeric(data_train[i,2:10]),na.rm = TRUE)}
colnames(data_train)[30] <- "No_Click"
```

###1. Plot of amount of transactions and contents
```{r}
#Maximum number of clicks for a customer is two, and minimun number of clicks for a customer is zero. 
ggplot(data=data_train)+
  aes(x=as.factor(No_Click),y=total_spend,fill=as.factor(No_Click))+
  geom_point(alpha=0.1)+
  geom_boxplot()+
  xlab("Number of Clicks")+
  ylab("Total Spend")+
  labs(title="Total Spend Amount vs. Number of Clicks",subtitle="No apparent difference in total spend amount range for class of number of clicks")+
  guides(fill=guide_legend(title="Number of Clicks"))
```

###2. Plot of number of transactions and contents
```{r}
ggplot(data=data_train)+
  aes(x=as.factor(No_Click),y=total_no,fill=as.factor(No_Click))+
  geom_point(alpha=0.1)+
  geom_boxplot()+
  xlab("Number of Clicks")+
  ylab("Number of Transactions")+
  labs(title="Number of Transactions vs. Number of Clicks",subtitle="No apparent difference in number of transactions range for class of number of clicks")+
  guides(fill=guide_legend(title="Number of Clicks"))
```

##Variables Correlation and Plot
```{r}
#Correlation plot for numeric variables 
#Select out numeric variables
M <- cor(data_train[,11:24])
#Correlation plot
corrplot(M,method = "color")
```
From the correlation plot, we can see that only the number of transactions and the total amount of spend from each type of store are correlated, and the variables intra-store type are not correlated at all (white on the plot).

##Part II: Modeling

In this Modeling section, I'll use the data deleted content_1=NA observations. 

###i. Multinomial Model for Contents 
The content_1 is orginally a three-level response variable, and the first relevant model that came to my mind is multinomial model.

I fit a multinomial model taking content one's clicking history (0, 1, NA: three levels) as response variable. In this case, the three possible responses are: not informed(NA) / informed but didn't click(0) / clicked(1) have an order.
To avoid problems, convert "NA" to "0", "0" to "1", "1" to "2". "0" means the customers are not informed, "1" means the customers didn't clicked, "2" means the customers have clicked.

```{r}
for (i in 1:9){
data_train[,1+i]<-  ifelse(is.na(data_train[,1+i])==TRUE,0,
                                ifelse(data_train[,1+i]=="0",1,
                                       ifelse(data_train[,1+i]=="1",2,NA)))
data_train[,1+i] <- as.factor(data_train[,1+i])
}

#Proceed the same value modification for test data
for (i in 1:9){
data_test[,1+i]<-  ifelse(is.na(data_test[,1+i])==TRUE,0,
                                ifelse(data_test[,1+i]=="0",1,
                                       ifelse(data_test[,1+i]=="1",2,NA)))
data_test[,1+i] <- as.factor(data_test[,1+i])
}
```

```{r}
#Model Fitting
ord_logit <- polr(content_1~as.factor(gender)+as.factor(affluency)+log(express.total.spend+1)+log(express.no.transactions+1)+log(metro.total.spend+1)+log(metro.no.transactions+1)+log(superstore.total.spend+1)+log(superstore.no.transactions+1)+log(extra.total.spend+1)+log(extra.no.transactions+1)+log(fandf.total.spend+1)+log(fandf.no.transactions+1)+log(petrol.total.spend+1)+log(petrol.no.transactions+1)+log(direct.total.spend+1)+log(direct.no.transactions+1),data=data_train,Hess = T)
summary(ord_logit)

#Residual Plots
resid_multinom <- model.matrix(~factor(content_1)-1, data=data_train)-fitted(ord_logit)
par(mfrow= c(2,2)) 
for (i in 1:3) {
binnedplot(fitted(ord_logit)[,i], resid_multinom[,i], cex.main=1.3, main="Binned residual plot", nclass = 20) }

#LogLoss
LogLoss(y_pred=ord_logit$fitted.values, y_true=as.numeric(data_train$content_1)) 

#Predict the test data
data_test[,-c(1:10,28,29)] -> data_test_multinom
predict_multinom_prob <- predict(ord_logit, newdata = data_test_multinom, "probs")
predict_multinom_class <- predict(ord_logit, newdata = data_test_multinom)
#Calculate the probability of mis-prediction
mean(as.character(predict_multinom_class) != as.character(data_test$content_1))
```
The multinomial model can be written as:
$$log[P(click=1)/P(click=0)]=0.11+\sum\beta*x$$ 
$$log[P(click=2)/P(click=1)]=5.24+\sum\beta*x$$
The only difference in these two formulas is the intercepts.

For this multinomial model:

1. The residual plots for three levels look acceptable, with most of the points located between two curves.

2. As for the coefficients, region and gender don't have a large influence on the response variable, neither numbers of transactions and total amounts, because their coefficient estimates are close to zero.

3. Prediction based on this model is a little bit better than random guess, about 55% of successful prediction rate. But the problem is that the model automatically take all predictions equal to 0.

I then move on to logistic model to see if the model will be more interpretable.

To simplify the problem, I then narrow the response's three levels to two levels by deleting NA observations in order to increase the ones proportion compared to the zeros.
"0" means the customers didn't clicked, "1" means the customers have clicked.

```{r}
#Reload original data
data_whole <- read.table(file = 'content_train.tsv', sep = '\t', header = TRUE)
data_whole%<>% mutate(total_spend=express.total.spend+metro.total.spend+
                      superstore.total.spend+extra.total.spend+fandf.total.spend+
                       petrol.total.spend+direct.total.spend)
data_whole%<>% mutate(total_no=express.no.transactions+metro.no.transactions+
                    superstore.no.transactions+extra.no.transactions+
                      fandf.no.transactions+
                      petrol.no.transactions+direct.no.transactions)
set.seed(2018)
test_index <-  sort(sample(nrow(data_whole), nrow(data_whole)*.2))
data_test <- data_whole[test_index,]
data_train <- data_whole[-test_index,]

#Preprocessing the data: deleting all NA observations
data_train_nona <- data_train %>% 
  filter(is.na(content_1)==FALSE)

data_test_nona <- data_test %>% 
  filter(is.na(content_1)==FALSE)

data_train_nona$content_1 <- as.factor(data_train_nona$content_1)

nrow(subset(data_train_nona,data_train_nona$content_1=="0"))
nrow(subset(data_train_nona,data_train_nona$content_1=="1"))

#Now for content 1, there are 36450 ones and 399 twos.
```

###ii. Logistic Model
```{r}
simp_logit <- glm(content_1~as.factor(gender)+as.factor(affluency)+log(express.total.spend+1)+log(express.no.transactions+1)+log(metro.total.spend+1)+log(metro.no.transactions+1)+log(superstore.total.spend+1)+log(superstore.no.transactions+1)+log(extra.total.spend+1)+log(extra.no.transactions+1)+log(fandf.total.spend+1)+log(fandf.no.transactions+1)+log(petrol.total.spend+1)+log(petrol.no.transactions+1)+log(direct.total.spend+1)+log(direct.no.transactions+1),data=data_train_nona,family = binomial)
summary(simp_logit)

#Evaluate the model
#Anova test to show if adding each variable makes sense
anova(simp_logit)

#LogLoss to see the difference between fitted values and actual values
LogLoss(y_pred=simp_logit$fitted.values, y_true=as.numeric(data_train_nona$content_1)) 

#Binned residual plot
binnedplot(x=simp_logit$fitted.values,y=resid(simp_logit,type="response"),nclass = 30)

#Error table
T_A <- mean(simp_logit$fitted.values<0.5 & data_train_nona$content_1==0)
T_B <- mean(simp_logit$fitted.values>0.5 & data_train_nona$content_1==0)
T_C <- mean(simp_logit$fitted.values<0.5 & data_train_nona$content_1==1)
T_D <- mean(simp_logit$fitted.values>0.5 & data_train_nona$content_1==1)
error_table <- matrix(c(T_A,T_B,T_C,T_D),byrow=TRUE,nrow = 2) 
rownames(error_table) <- c("y=0","y=1")
colnames(error_table) <- c("y_hat=0","y_hat=1")
error_table

#Predictions
data_test[,-c(1:10,28,29)] %>% filter(is.na(data_test$content_1)==FALSE)-> data_test_glm
predict_glm_class <- predict(simp_logit, newdata = data_test_glm)
probs_glm <- exp(predict_glm_class)/(1+exp(predict_glm_class)) #probability of 1
data_test_use <- data_test %>% filter(is.na(data_test$content_1)==FALSE)
ggplot(data_test_use)+
  aes(x=probs_glm,y=as.factor(data_test_use$content_1))+
  geom_point()+
  xlab("Predicted Probability")+ylab("Actual Value")
```

For this logistic model:
1. Comapring the null deviance and residual deviance, we can see that the model has been improved by adding new variables.
2. About the coefficients, gender is the most influencial variable to the response variable, and the others are still not explaining it well.
3. From the ANOVA test, even though the deviance is not improving drastically, as I added each variable, the deviance does decrease (but of small amount).
4. LogLoss is a value that we want to minimize signifying that the model is more accurate. The LogLoss in this model is higher than that of multinomial model.
5. Binned residual plot here looks normal, with all points between lines and symmetrically distributed above and below zero.
6. The error table is telling a big problem, as the one I've observed from the multinomial model before. The model is having a really high successful prediction rate, 99.5%. Simply because that it is predicting all the test observations to be zero, because the original training dataset has only less than 0.05% which has $content_1=1$ The number of customers who have clicked through is really low. I'll deal with this in the next section "Rare event problem".
7. The prediction plot also illustrate the last problem. We have the predicted probability not higher than 0.02, which we can approximate to a probabilitu of zero, but there are a number of points of one.


To deal with logistic regression's "rare event problem":
We only have 399 "ones" in data_train for content_1, which is relatively rare, about 1%. I'll try three most popular methods to deal with this kind of "Rare Event Problem":

####a. brglm: Bias Reduction in Binomial-Response Generalized Linear Models
```{r}
brglm_logit <- brglm(content_1~as.factor(gender)+as.factor(affluency)+log(express.total.spend+1)+log(express.no.transactions+1)+log(metro.total.spend+1)+log(metro.no.transactions+1)+log(superstore.total.spend+1)+log(superstore.no.transactions+1)+log(extra.total.spend+1)+log(extra.no.transactions+1)+log(fandf.total.spend+1)+log(fandf.no.transactions+1)+log(petrol.total.spend+1)+log(petrol.no.transactions+1)+log(direct.total.spend+1)+log(direct.no.transactions+1),data=data_train_nona,family = binomial)
summary(brglm_logit)

#Residual Plot
binnedplot(x=brglm_logit$fitted.values,y=resid(brglm_logit,type="response"),nclass = 30)

#Error table
T_A <- mean(fitted(brglm_logit)<0.5 & data_train_nona$content_1==0)
T_B <- mean(fitted(brglm_logit)>0.5 & data_train_nona$content_1==0)
T_C <- mean(fitted(brglm_logit)<0.5 & data_train_nona$content_1==1)
T_D <- mean(fitted(brglm_logit)>0.5 & data_train_nona$content_1==1)
error_table <- matrix(c(T_A,T_B,T_C,T_D),byrow=TRUE,nrow = 2) 
rownames(error_table) <- c("y=0","y=1")
colnames(error_table) <- c("y_hat=0","y_hat=1")
error_table

#Predictions
data_test[,-c(1:10,28,29)] %>% filter(is.na(data_test$content_1)==FALSE)-> data_test_glm
predict_brglm_class <- predict(brglm_logit, newdata = data_test_glm)
probs_brglm <- exp(predict_brglm_class)/(1+exp(predict_brglm_class)) #probability of 1
ggplot(data_test_use)+
  aes(x=probs_brglm,y=data_test_use$content_1)+
  geom_point()+
  xlab("Predicted Probability")+ylab("Actual Value")
```

From this brglm model:
1. We're seeing similar model outputs in summary, given that I've kept all the same independent variables.
2. The error table shows similar results and the same problem as before; So does the prediction test.
3. This model is not a good remedy for our problem in the context.


####b. logistf: Firth’s bias-Reduced penalized-likelihood logistic regression
```{r}
stf_logit <- logistf(content_1~as.factor(gender)+as.factor(affluency)+log(express.total.spend+1)+log(express.no.transactions+1)+log(metro.total.spend+1)+log(metro.no.transactions+1)+log(superstore.total.spend+1)+log(superstore.no.transactions+1)+log(extra.total.spend+1)+log(extra.no.transactions+1)+log(fandf.total.spend+1)+log(fandf.no.transactions+1)+log(petrol.total.spend+1)+log(petrol.no.transactions+1)+log(direct.total.spend+1)+log(direct.no.transactions+1),data=data_train_nona)
summary(stf_logit)

#Error table
T_A <- mean(stf_logit$predict<0.5 & data_train_nona$content_1==0)
T_B <- mean(stf_logit$predict>0.5 & data_train_nona$content_1==0)
T_C <- mean(stf_logit$predict<0.5 & data_train_nona$content_1==1)
T_D <- mean(stf_logit$predict>0.5 & data_train_nona$content_1==1)
error_table <- matrix(c(T_A,T_B,T_C,T_D),byrow=TRUE,nrow = 2) 
rownames(error_table) <- c("y=0","y=1")
colnames(error_table) <- c("y_hat=0","y_hat=1")
error_table

#Predictions 
betas <- coef(stf_logit)
formula <- content_1~as.factor(gender)+as.factor(affluency)+log(express.total.spend+1)+log(express.no.transactions+1)+log(metro.total.spend+1)+log(metro.no.transactions+1)+log(superstore.total.spend+1)+log(superstore.no.transactions+1)+log(extra.total.spend+1)+log(extra.no.transactions+1)+log(fandf.total.spend+1)+log(fandf.no.transactions+1)+log(petrol.total.spend+1)+log(petrol.no.transactions+1)+log(direct.total.spend+1)+log(direct.no.transactions+1)
data_test_stf <- data_test[,-c(1,3:10,28,29)] %>% filter(is.na(data_test$content_1)==FALSE)
X <- model.matrix(formula, data=data_test_stf) 
pi.obs <- 1 / (1 + exp(-X %*% betas)) 
ggplot(data_test_use)+
  aes(x=pi.obs,y=data_test_use$content_1)+
  geom_point()+
  xlab("Predicted Probability")+ylab("Actual Value")
```

From this logitstf model:
1. The error table has the totally opposite result compared to the logistic model, which is not normal.
2. Prediction plot shows the same conclusion: the predicted probability higher than 0.99 but the majority of actual points are zeros.


####c. stan_glm: Use MCMC method to fit logistic model 

Bayesian method is also mentioned while talking about "Rare Event Problem", because MCMC method is different from Maximum Likelihood Estimation process. 
```{r}
library(rstanarm)
simp_logit_stan <- stan_glm(content_1~as.factor(gender)+as.factor(affluency)+log(express.total.spend+1)+log(express.no.transactions+1)+log(metro.total.spend+1)+log(metro.no.transactions+1)+log(superstore.total.spend+1)+log(superstore.no.transactions+1)+log(extra.total.spend+1)+log(extra.no.transactions+1)+log(fandf.total.spend+1)+log(fandf.no.transactions+1)+log(petrol.total.spend+1)+log(petrol.no.transactions+1)+log(direct.total.spend+1)+log(direct.no.transactions+1),
                            data=data_train_nona,
                            family = binomial(link = "logit"),
                            iter=1000,chains=2,
                            prior = )

print(simp_logit_stan)
pp_check(simp_logit_stan,plotfun = "intervals")
```
Limited to computational capability, I've set number of chains to 2 and number of iterations to 1000. 
Based on ppcheck plot, which is similar to the prediction plot, the problem still take place, because model's return values are all located at zero while we have some data scatters at one. 

In general, these three models designed to tackle rare data problem didn't work out in this context, and I will be open to explore other solutions to this problem.

###iii. Multilevel Mixed Effect Model for Contents

It would also make sense to try multilevel mixed effect model. I'll put gender and affluency, the only two factor variables in the data as random effects.

```{r}
mix_logit <- glmer(content_1~(1|gender)+(1|affluency)+
                     scale(express.total.spend)+scale(metro.total.spend)+
                    scale(superstore.total.spend)+scale(extra.total.spend)+
                    scale(fandf.total.spend)+scale(petrol.total.spend)+
                    scale(direct.total.spend),
                   data=data_train_nona,family = binomial,REML=FALSE) 
summary(mix_logit)
ranef(mix_logit)

#Residual Plot
binnedplot(fitted(mix_logit),resid(mix_logit,type = "response"),nclass =30)

#Predictions
PI <- predictInterval(merMod = mix_logit, newdata = data_test_use,
                        level = 0.95, 
                        stat = "median", type="linear.prediction",
                        include.resid.var = TRUE)
probs_glmer <- matrix(nrow = 9173,ncol=3) #Transfer value to probability 
for(i in 1:3){
probs_glmer[,i] <- exp(PI[,i])/(1+exp(PI[,i])) #probability of 1
}
colnames(probs_glmer) <- colnames(PI)
probs_glmer <- as.data.frame(probs_glmer)
#Plot only on the first 30 observations 
ggplot(aes(x=1:30, y=fit, ymin=lwr, ymax=upr), data=probs_glmer[1:30,]) +
  geom_point() +
  geom_linerange() +
  labs(x="Index", y="Prediction w/ 95% PI") + theme_bw()
#Plot predicted value vs. actual value
ggplot(data=data_test_use,aes(x=probs_glmer[,1],y=data_test_use$content_1))+
  geom_point()+
  xlab("Predicted Probability")+ylab("Actual Value")

#Error table
T_A <- mean(probs_glmer[,1]<0.5 & data_train_nona$content_1==0)
T_B <- mean(probs_glmer[,1]>0.5 & data_train_nona$content_1==0)
T_C <- mean(probs_glmer[,1]<0.5 & data_train_nona$content_1==1)
T_D <- mean(probs_glmer[,1]>0.5 & data_train_nona$content_1==1)
error_table <- matrix(c(T_A,T_B,T_C,T_D),byrow=TRUE,nrow = 2) 
rownames(error_table) <- c("y=0","y=1")
colnames(error_table) <- c("y_hat=0","y_hat=1")
error_table
```

From this multilevel mixed effect model:
1. The affluency as a random effect doesn't have a high influence on the model, with its coefficients really close to zero. While the gender effect relatively has a large difference. So I don't think that adding levels to the model help me explain more about the response variable.
2. Binned residual plot of this model looks normal.
3. The predictions made on test set and the error table seem that it's still suffering the same problem left before, known as "rare event" problem. The three models (multinomial,logistic and multilevel) are showing exactly same results in error table and predictions.


###iv. Models After Modifying Proportions Of 0s and 1s

Modifying the content_1 to 50% of ones and 50% of zeros, which may produce a more "real" prediction accuracy rate, but this data processing will lose the proportions' information and I'm not sure if it will make sense because one of the most important messages conveyed from the data is the proportion of ones in content clicking choice. 

```{r}
#Length of ones in content_1
length_ones <- nrow(subset(data_train_nona,data_train_nona$content_1==1))
#I'll random pick 399 zeros observations from the data
set.seed(2018)
data_train_fifty <- data_train_nona[sort(sample(1:nrow(data_train_nona),length_ones)),]
data_train_fifty <- rbind(data_train_fifty,subset(data_train_nona,data_train_nona$content_1==1))
```  

```{r}
simp_logit_fifty <- glm(content_1~as.factor(gender)+as.factor(affluency)+log(express.total.spend+1)+log(express.no.transactions+1)+log(metro.total.spend+1)+log(metro.no.transactions+1)+log(superstore.total.spend+1)+log(superstore.no.transactions+1)+log(extra.total.spend+1)+log(extra.no.transactions+1)+log(fandf.total.spend+1)+log(fandf.no.transactions+1)+log(petrol.total.spend+1)+log(petrol.no.transactions+1)+log(direct.total.spend+1)+log(direct.no.transactions+1),data=data_train_fifty,family = binomial)
summary(simp_logit_fifty)

#Binned residual plot
binnedplot(x=simp_logit_fifty$fitted.values,y=resid(simp_logit_fifty,type="response"),nclass = 30)

#Added variable plot
anova(simp_logit_fifty)

#Model assumptions check 
plot(simp_logit_fifty)

#Error table
T_A <- mean(simp_logit_fifty$fitted.values<0.5 & data_train_fifty$content_1==0)
T_B <- mean(simp_logit_fifty$fitted.values>0.5 & data_train_fifty$content_1==0)
T_C <- mean(simp_logit_fifty$fitted.values<0.5 & data_train_fifty$content_1==1)
T_D <- mean(simp_logit_fifty$fitted.values>0.5 & data_train_fifty$content_1==1)
error_table <- matrix(c(T_A,T_B,T_C,T_D),byrow=TRUE,nrow = 2) 
rownames(error_table) <- c("y=0","y=1")
colnames(error_table) <- c("y_hat=0","y_hat=1")
error_table

#Predictions
#In prediction, I'll still use the whole test set without  manipulating the proportions of zeros and ones on the test data. 
data_test[,-c(1:10,28,29)] %>% filter(is.na(data_test$content_1)==FALSE)-> data_test_glm
predict_glm_class <- predict(simp_logit_fifty, newdata = data_test_glm)
probs_glm <- exp(predict_glm_class)/(1+exp(predict_glm_class)) #probability of 1
data_test_use <- data_test %>% filter(is.na(data_test$content_1)==FALSE)
ggplot(data_test_use)+
  aes(x=probs_glm,y=as.factor(data_test_use$content_1))+
  geom_point()+
  xlab("Predicted Probability")+ylab("Actual Value")+
  geom_vline(xintercept=0.5,color="red")

```

From this logistic model:
1. We're not confident that the variables put in the model are explaining the response variable well, which may decrease the prediction accuracy; 
2. The anova test results tell us that adding some variables (such as superstore.no.transactions, extra.no.transactions, etc) didn't improve the model's explainability a lot;
3. From model assumption check plots, the model's residuals are not normal distributed(points are far from located on the line); the residuals aren't spread equally along the ranges of predictors(a weird cross on the plot); and no influencial cases in this case;
4. The binned residual plot proves the absence of heteroscedasticity;
5. With modified data, the error table returns a "normal" result, 63% accuracy rate in train data. And the prediction plot's x-axis, prediction values, now has a range from 0 to 1 (compared to 0 to 0.1 before), and it means that the data modification solves the problem suffered before;
6. This result also prove on the one side, the data will cause "rare event" problem to the models, even though this model result (after modifying data's proportion) is limited in usage; On the other side, the fact that previous models don't explain the response variable well is not largely due to the "rare event" problem, it's due to the variables' selection problem.


#E. Discussion

##i. Implitcation

From all the results and the interpretations of the models fitted above, it's not reasonable to use these models to predict clicking rate; while the models do explain the clicking rate to some extend.


##ii. Limitation

1. The "rare event" problem is not resolved in this project, which will constraint the models proceeding predictions.
2. The data is random simulated by the company with some latent assumptions, which are not disclosed to data users. These assumptions are supposed to be discovered by learning results, but in my project, they have not been identified. This can potentialy be solved when I've learnt more about machine learning.

##iii. Future Directions

1. I may continue on looking for other methods which could resolve the rare event problem in this dataset.
2. The data's structure or other latent relations may violate the assumptions of regressions I'm applying in the project. Therefore, some other more advanced learning methods may be more appropriate for this data's predicting problem.
3. After finding a more proper and efficient way to make predictions, I will continue on predicting content_2 to content_9. 


# Appendix:

##I. Other EDA Plots 

###1. Total amount of transactions vs. Affluency
```{r}
#Group data 
plot_amt <- data_train %>% 
  dplyr::select(affluency,express.total.spend,metro.total.spend,                       superstore.total.spend,extra.total.spend,fandf.total.spend,
                      petrol.total.spend,direct.total.spend) %>% 
  group_by(affluency) %>% summarise(Express=sum(express.total.spend),
                                    Metro=sum(metro.total.spend),
                                    Superstore=sum(superstore.total.spend),
                                    Extrastore=sum(extra.total.spend),
                                    Fandf=sum(fandf.total.spend),
                                    Petrol=sum(petrol.total.spend),
                                    Direct=sum(direct.total.spend))

plot_mat <- matrix(0,nrow = 35,ncol = 3)
plot_mat[,1] <- rep(c("High","Low","Mid","Very High","Very Low"),7)
plot_mat[,2] <- c(rep(x="Express",5),rep(x="Metro",5),
                  rep(x="Superstore",5),rep(x="Extrastore",5),
                  rep(x="Fandf",5),rep(x="Petrol",5),rep(x="Direct",5))
plot_mat[,3] <- rbind(plot_amt$Express,plot_amt$Metro,plot_amt$Superstore,plot_amt$Extrastore,
                      plot_amt$Fandf,plot_amt$Petrol,plot_amt$Direct)
plot_mat <- as.data.frame(plot_mat)
colnames(plot_mat) <- c("Affluency","Store.Type","Total.Amount")
plot_mat$Total.Amount <- as.numeric(plot_mat$Total.Amount)

#Plot
g1 <- 
  ggplot(data = plot_mat)+
  aes(x=as.factor(Affluency),y=Total.Amount,fill=as.factor(Store.Type))+
  geom_col(position = 'stack')+
  scale_x_discrete(limits=c("Very Low","Low","Mid","High","Very High"))+
  xlab("Affluency")+
  ylab("Total Amount")+
  scale_fill_discrete(name="Store Type")
g2 <- 
  ggplot(data = plot_mat)+
  aes(x=as.factor(Affluency),y=Total.Amount,fill=as.factor(Store.Type))+
  geom_col(stat="identity", position="fill")+
  scale_x_discrete(limits=c("Very Low","Low","Mid","High","Very High"))+
  xlab("Affluency")+
  ylab("Proportion")+
  scale_fill_discrete(name="Store Type")+
  coord_flip()
grid.arrange(g1, g2)
```

###2. Number of transactions vs. Affluency
```{r}
#Group data 
data_train %>% dplyr::select(affluency,express.no.transactions,metro.no.transactions,
                    superstore.no.transactions,extra.no.transactions,fandf.no.transactions,
                       petrol.no.transactions,direct.no.transactions) %>% 
  group_by(affluency) %>% summarise(Express=sum(express.no.transactions),
                                    Metro=sum(metro.no.transactions),
                                    Superstore=sum(superstore.no.transactions),
                                    Extrastore=sum(extra.no.transactions),
                                    Fandf=sum(fandf.no.transactions),
                                    Petrol=sum(petrol.no.transactions),
                                    Direct=sum(direct.no.transactions))->plot_amt

plot_mat <- matrix(0,nrow = 35,ncol = 3)
plot_mat[,1] <- rep(c("High","Low","Mid","Very High","Very Low"),7)
plot_mat[,2] <- c(rep(x="Express",5),rep(x="Metro",5),
                  rep(x="Superstore",5),rep(x="Extrastore",5),
                  rep(x="Fandf",5),rep(x="Petrol",5),rep(x="Direct",5))
plot_mat[,3] <- rbind(plot_amt$Express,plot_amt$Metro,plot_amt$Superstore,plot_amt$Extrastore,
                      plot_amt$Fandf,plot_amt$Petrol,plot_amt$Direct)
plot_mat <- as.data.frame(plot_mat)
colnames(plot_mat) <- c("Affluency","Store.Type","No.Amount")
plot_mat$No.Amount <- as.numeric(plot_mat$No.Amount)

#Plot
g1 <- 
  ggplot(data = plot_mat)+
  aes(x=as.factor(Affluency),y=No.Amount,fill=as.factor(Store.Type))+
  geom_col(position = 'stack')+
  scale_x_discrete(limits=c("Very Low","Low","Mid","High","Very High"))+
  xlab("Affluency")+
  ylab("Number of Transactions")+
  scale_fill_discrete(name="Store Type")+
  coord_flip()
g2 <- 
  ggplot(data = plot_mat)+
  aes(x=as.factor(Affluency),y=No.Amount,fill=as.factor(Store.Type))+
  geom_col(stat="identity", position="fill")+
  scale_x_discrete(limits=c("Very Low","Low","Mid","High","Very High"))+
  xlab("Affluency")+
  ylab("Proportion")+
  scale_fill_discrete(name="Store Type")+
  coord_flip()
grid.arrange(g1, g2)
```

###3. Total amount of transactions vs. Gender
```{r}
#Group data 
data_train %>% dplyr::select(gender,express.total.spend,metro.total.spend,
                      superstore.total.spend,extra.total.spend,fandf.total.spend,
                       petrol.total.spend,direct.total.spend) %>% 
  group_by(gender) %>% summarise(Express=sum(express.total.spend),
                                    Metro=sum(metro.total.spend),
                                    Superstore=sum(superstore.total.spend),
                                    Extrastore=sum(extra.total.spend),
                                    Fandf=sum(fandf.total.spend),
                                    Petrol=sum(petrol.total.spend),
                                    Direct=sum(direct.total.spend))->plot_amt

plot_mat <- matrix(0,nrow = 14,ncol = 3)
plot_mat[,1] <- rep(c("Male","Female"),7)
plot_mat[,2] <- c(rep(x="Express",2),rep(x="Metro",2),
                  rep(x="Superstore",2),rep(x="Extrastore",2),
                  rep(x="Fandf",2),rep(x="Petrol",2),rep(x="Direct",2))
plot_mat[,3] <- rbind(plot_amt$Express,plot_amt$Metro,plot_amt$Superstore,plot_amt$Extrastore,
                      plot_amt$Fandf,plot_amt$Petrol,plot_amt$Direct)
plot_mat <- as.data.frame(plot_mat)
colnames(plot_mat) <- c("Gender","Store.Type","Total.Amount")
plot_mat$Total.Amount <- as.numeric(plot_mat$Total.Amount)

#Plot
g1 <- 
  ggplot(data = plot_mat)+
  aes(x=as.factor(Gender),y=Total.Amount,fill=as.factor(Store.Type))+
  geom_col(position = 'stack')+
  xlab("Gender")+
  ylab("Total Amount")+
  scale_fill_discrete(name="Store Type")+
  coord_flip()
g2 <- 
  ggplot(data = plot_mat)+
  aes(x=as.factor(Gender),y=Total.Amount,fill=as.factor(Store.Type))+
  geom_col(stat="identity", position="fill")+
  xlab("Gender")+
  ylab("Proportion")+
  scale_fill_discrete(name="Store Type")+
  coord_flip()
grid.arrange(g1, g2)
```

###4. Number of transactions vs. Gender
```{r}
#Group data 
data_train %>% select(gender,express.no.transactions,metro.no.transactions,
                       superstore.no.transactions,extra.no.transactions,fandf.no.transactions,
                       petrol.no.transactions,direct.no.transactions) %>% 
  group_by(gender) %>% summarise(Express=sum(express.no.transactions),
                                    Metro=sum(metro.no.transactions),
                                    Superstore=sum(superstore.no.transactions),
                                    Extrastore=sum(extra.no.transactions),
                                    Fandf=sum(fandf.no.transactions),
                                    Petrol=sum(petrol.no.transactions),
                                    Direct=sum(direct.no.transactions))->plot_amt

plot_mat <- matrix(0,nrow = 14,ncol = 3)
plot_mat[,1] <- rep(c("Male","Female"),7)
plot_mat[,2] <- c(rep(x="Express",2),rep(x="Metro",2),
                  rep(x="Superstore",2),rep(x="Extrastore",2),
                  rep(x="Fandf",2),rep(x="Petrol",2),rep(x="Direct",2))
plot_mat[,3] <- rbind(plot_amt$Express,plot_amt$Metro,plot_amt$Superstore,plot_amt$Extrastore,
                      plot_amt$Fandf,plot_amt$Petrol,plot_amt$Direct)
plot_mat <- as.data.frame(plot_mat)
colnames(plot_mat) <- c("Gender","Store.Type","No.Amount")
plot_mat$No.Amount <- as.numeric(plot_mat$No.Amount)

#Plot
g1 <- 
  ggplot(data = plot_mat)+
  aes(x=as.factor(Gender),y=No.Amount,fill=as.factor(Store.Type))+
  geom_col(position = 'stack')+
  xlab("Gender")+
  ylab("Number of Transactions")+
  scale_fill_discrete(name="Store Type")+
  coord_flip()
g2 <- 
  ggplot(data = plot_mat)+
  aes(x=as.factor(Gender),y=No.Amount,fill=as.factor(Store.Type))+
  geom_col(stat="identity", position="fill")+
  xlab("Gender")+
  ylab("Proportion")+
  scale_fill_discrete(name="Store Type")+
  coord_flip()
grid.arrange(g1, g2)
```

##II. Another Method on Data Preprocessing

I've first use the data with all observations to fit all three regression models, which means the NAs have not been removed. The results were similar: all three model's explaning abilities on the content_1 clicking rate were good but the models predicted each output (no matter the values of variables) to be zero, because of the rare event problem, which means that the training dataset has overwhelming amount of zeros compared to ones for content_1. I then think of deleting NA at the beginning, because after deleting the NAs, the proportion of ones will increase a little bit (but finally found out that this was not sufficient).

##III. Statistical Learning Methods Used in the Project

For this project, after getting these results, I've applied some learning methods which can potentially be helpful:
1. Gradient Boosting Desicion Tree: https://www.r-bloggers.com/gradient-boosting-in-r/ 
It was used to deal with the "rare event" problem, but I had a hard time interpreting the results and I was not sure about if the whole process was correct or not, so I didn't put it into the report.

2. Neural Network: https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/
Depending on my understanding, neural network can handle some latent, complexe and multi-layer relations, which may be useful on identig the company's inital assumptions on the simulations. But I think I need to spend more time learning its concepts and package to have a whole picture understanding before applying it to data.


# References:

[1]: https://turi.com/learn/gallery/notebooks/click_through_rate_prediction_intro.html
[2]: https://www3.nd.edu/~rwilliam/stats3/rareevents.pdf
[3]: https://www.r-bloggers.com/example-8-15-firth-logistic-regression/
[4]: https://www.r-bloggers.com/making-sense-of-logarithmic-loss/
[5]: https://cran.r-project.org/web/packages/brglm/brglm.pdf
[6]: https://stats.stackexchange.com/questions/354084/modelling-rare-events-with-small-sample-size
